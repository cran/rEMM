\documentclass[fleqn, letter, 10pt]{article}
\usepackage[round,longnamesfirst]{natbib}
\usepackage[left=3cm,top=3cm,right=3cm,bottom=3cm,nohead]{geometry} 
\usepackage{graphicx,keyval,thumbpdf,url}
\usepackage{hyperref}
\usepackage{Sweave}
\SweaveOpts{strip.white=TRUE, eps=FALSE}
\AtBeginDocument{\setkeys{Gin}{width=0.6\textwidth}}


\usepackage[utf8]{inputenc}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsfonts}


\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\sQuote}[1]{`{#1}'}
\newcommand{\dQuote}[1]{``{#1}''}
\newcommand\R{{\mathbb{R}}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\setlength{\parindent}{0mm}
\setlength{\parskip}{3mm plus2mm minus2mm}

%% \VignetteIndexEntry{An Implementation of EMM for R}


\begin{document}

\title{rEMM: An Implementation of EMM for \proglang{R}}
\author{Michael Hahsler and Magaret H. Dunham}
%\date{March 4, 2009}
\maketitle
%\tableofcontents
\sloppy


\abstract{Abstract goes here}

<<echo=FALSE>>=
options(width = 70, prompt="R> ")
### for sampling
set.seed(1234)
@


\section{Introduction}
Clustering data streams has become an important
field in recent years. Data streams are generated by many types of applications
including web click-stream data, computer network monitoring data,
telecommunication connection data, readings from sensor nets, stock quotes, etc.
The most important property of data streams
for clustering is that data streams contain massive amounts of data which has
to processed in time and possibly cannot be permanently stored (transient
data). This leads to the following requirements:

\begin{itemize}
\item Data can only be processed in a single pass or scan of the data.
\item Only a minimal amount of data can be retained and the clusters have to 
be represented in a extremely concise way.
\item Data stream characteristics may change over time 
(e.g., clusters move, merge, disappear or
new clusters may appear).
\end{itemize}

Many algorithms for stream clustering have been proposed.

\cite{stream_clust:O'Callaghan:2002}...

For example, \cite{stream_clust:Guha:2003} study the $k$-median 
problem of clustering. Their algorithm divides the data stream into pieces,
clusters each piece individually and then iteratively recluster the resulting
centers to obtain a final clustering.
\cite{stream_clust:Aggarwal:2003} present {\em CluStream} which uses
micro cluster (a temporal extension of cluster feature vectors used
by BIRCH~\citep{stream_clust:Zhang:1996}). 
Micro clusters can be deleted and merged and stored
at different points in time to allow to create final clustering (recluster
micro clusters with $k$-means) for different time frames.
\cite{stream_clust:Kriegel:2003} and \cite{stream_clust:Tasoulis:2007} present
variants of the density based method 
{\em OPTICS}~\citep{stream_clust:Ankerst:1999} suitable for streaming data.
\cite{stream_clust:Aggarwal:2004} introduce {\em HPStream} which finds 
clusters that are well defined in different subsets of the dimensions
of the data. The set of dimensions for each cluster can evolve over time 
and a fading function is used to discount the influence of older data points,
by fading the entire cluster structure.
\cite{stream_clust:Cao:2006} introduce {\em DenStream} which maintains 
micro clusters in real time and uses a variant of 
GDBSCAN~\citep{stream_clust:Sander:1998} to produce a final clustering 
for users.
\cite{stream_clust:Tasoulis:2006} present {\em WSTREAM,} which using 
kernel density estimation to find rectangular windows to represent clusters.
The windows can move, contract, expand and be merged over time. 
%\cite{stream_clust:Aggarwal:2008} Uncertain Data
%\cite{stream_clust:Aggarwal:2009} Massive Data

All approaches center on finding clusters of data points but neglect the
temporal structure of the data which might be crucial important to 
understand the
underlying data. For example, in intrusion detection a user might change from
behavior A to behavior B, both represented by clusters labeled non-suspicious
behavior, but the transition form A to B might be extremely unusual and give
away an intrusion. The Extensible Markov Model \citep[EMM,][]{emm:Dunham:2004}
provides a micro cluster-based data stream clustering method which retains
temporal information in form of an evolving Markov Chain with clusters as
states and transitions representing the temporal information.
EMM was successfully applied to rare event and intrusion
detection~\citep{emm:Meng:2006c, emm:Isaksson:2006, emm:Meng:2006a}, web usage
mining~\citep{emm:Lu:2006}, and identifying emerging events and developing
trends~\citep{emm:Meng:2006, emm:Meng:2006b}.
\marginpar{non-stationary Markov process}

The paper is organized as follows....

\section{Extensible Markov Model}

EMM can be seen as an evolving Markov Chain (MC) which at each point in time
represents a regular time-homogeneous MC but is updated when new data is
available. In the following we will restrict the discussion to first order MMs
but it is straight forward to extend EMC to higher order models. 

\marginpar{add ref}
A (first order) Markov Chain is a sequence of random variables
$X_1, X_2, \dots$. All random variables have the same domain
$\mathrm{dom}(X_i) = S =\{s_1, s_2, \dots, s_k\}$, a countable 
set called the state space. The Markov
property states that the next state is only dependent on the current state.
Formally,

\begin{equation}
P(X_{l+1} = s| X_l =s_l, \dots, X_1=s_1) = P(X_{l+1} = s| X_l =s_l)
\end{equation}

For simplicity we use for transition probabilities the notation 
$a_{ij} = P(X_{l+1} = s_i| X_l =s_j)$ 
where it is appropriate.  Time-homogeneous MC can
be represented by a so called transition matrix containing the transition
probabilities from each state to all other state.

\begin{equation}
\mathbf{A} =
\begin{pmatrix}
a_{11}&a_{12}&\dots&a_{1k}\\
a_{21}&a_{22}&\dots&a_{2k}\\
\vdots&\vdots&\ddots&\vdots\\
a_{k1}&a_{k2}&\dots&a_{kk}\\
\end {pmatrix}
\end{equation}

MCs are a perfect way to keep track of temporal information using the Markov
Property as a relaxation.  With a MC is is easy to forecast the probability of
future states. For example the probability to get from a state to any other
state in $n$ time steps is given by the matrix $\mathbf{A}^n$. With an MC is
also easy to calculate the probability (likelihood???) of a new sequence by

\begin{equation}
P(X_l=s_l, X_{l-1}=s_{l-1} \dots, X_1=s_1) = 
    \prod_{i=1}^{l-1}{P(X_{i+1} = s_{i+1}| X_{i} =s_i)}
\end{equation}

The probabilities of a Markov Chain can be directly estimated from the 
data using the maximum likelihood method by
\begin{equation}
a_{ij} = c_{ij}/n_i,
\end{equation}
where $c_{ij}$ is the count of transitions from $s_i$ to $s_j$ in the data
and $n_i=\sum{c_{i.}}$.

Data streams typically contain dimensions with continuous data and/or have
discreet dimensions with a large number of domain
values~\citep{stream_clust:Aggarwal:2009}.  Therefore the data points have to
be mapped onto a manageable number of states first. This mapping is done by
clustering. Each cluster (or microcluster) is the represented 
by a state in the MC. The
transition information is maintained during the clustering process by using an
additional data structure representing the MC as a graph which can also be
incorporated into the cluster information which is managed by the clustering
algorithm. Since it only uses information (assignment of a data point to a
cluster) which is created by the clustering algorithm anyway, the computational
overhead is minimal.  When the clustering algorithm creates, merges or deletes
clusters, the corresponding state in the MC automatically also gets created,
merged or deleted resulting in an evolving MC, called an EMM.

Many existing clustering algorithms can be easily 
extended to maintain an EMM. For example,
BIRCH~\citep{stream_clust:Zhang:1996},
CluStream~\citep{stream_clust:Aggarwal:2003},
DenStream~\citep{stream_clust:Cao:2006} or
WSTREAM~\citep{stream_clust:Tasoulis:2006} can be used.

In the following we look at the additional data structures and the operations
on these structure which are necessary to maintain an EMM by an existing
clustering algorithm. But first we need to introduce some notation. We define
$S=\{s_1, s_2,\dots,s_k\}$ to be the set of clusters/states 
where $k$ is the current number of clusters.  
$\mathbf{n} = (n_1, n_2,\dots,n_k)$ is a vector with $n_i$ being
the number of data points assigned to cluster $s_i$.
The current state $s_c \in \{\epsilon, 1,2,\dots,k\}$ is either
no state~($\epsilon$; for the first data point) or one of the $k$ clusters.
\marginpar{explain $\epsilon$}

\paragraph{Data structures for the EMM.}
Typically a clustering algorithm for stream data uses a very sparse
represerntation for each cluster consisting of a way to summarize the center
and the dispersion of the data as well as how many data points were assigned to
the cluster so far.  Since the cluster represents a state in the EMM we need to
add a data structure to store the outgoing edges and their counts.
For cluster $C_i$ we need to store a count vector $c_{i.}$.
Together with $n_i$, the number of data points assigned to the cluster, 
it is easy to calculate the estimated transition probabilities 
by $a_{i.}=c_{i.}/n_i$.

An easy way manage all transition counts is to store all count vector
in a count matrix $\mathbf{C}$. Now it is easy to calculate the 
transition probability matrix by
\begin{equation}
\mathbf{A} = \mathbf{C} / \mathbf{n}
\end{equation}
A typical EMM is a sparse graph and the count vectors $c_{i.}$ stored with the
clusters contains zeros. Storing the count matrix in sparse format helps to
reduce space requirements significantly.

For the EMM we keep track of the current state $s_c$ using a single variable
containing a cluster identifier.

\marginpar{Concurrent streams with multiple current states.}

\paragraph{Adding a data point to an existing cluster.}

When a data point is added to an existing cluster $s_i$, the EMM has to
update the transition count from the current state to the new state by
setting $c_{s_c,i} = c_{s_c,i} +1$.
Finally the current state is set to the new state by $s_c=i$.

\paragraph{Creating a new cluster.}

Whenever the clustering algorithm creates a new (micro)cluster $s_{k+1}$,
the transition count matrix $\mathbf{C}$ has to be enlarged by a row and a 
column. Since the matrix is stored in sparse format, this modification
incurrs only minimal work. Then we can use the same operation as 
adding a data point to an existng cluster described above.

\paragraph{Deleting clusters.}

When a cluster $s_i$ (typically an outlier cluster) is deleted by 
the clustering algorithm, all we need to do is to remove 
the row $i$ and column $i$ in the transition count matrix $\mathbf{C}$.
This deletes the state.

\paragraph{Merging clusters.}

When two clusters~$s_i$ and $s_j$ are merged into a new cluster $s_m$, 
we need to: 

\begin{enumerate}
\item create new state $s_m$ in $\mathbf{C}$.
\item update the outgoing edges for $s_m$  by $c_{m.} = c_{i.} +c_{j.}$.
\item update the incomming edges for $s_m$ by $c_{.m} = c_{.i} + c_{.j}$.
\item delete columns and rows for the  old states $s_i$ and $s_j$ 
    from $\mathbf{C}$ (see deleting clusters above).
\end{enumerate}

Everything else is typically handled by the clustering algorithm.
It is straight forwardto extend the merge to an arbitrary number of clusters
at a time.

Merging states also covers reclustering which is done by many stream clustering
algorithm to create a final clustering for the user/application.


\paragraph{Splitting clusters.}

Splitting microclusters is typically not implemented in 
data stream clustering algorithms since the individual data points are not 
stored and therefore it is hard to find two new meaningful clusters. 
When clusters are split by algorithms like BIRCH, it typically means that
one or several microclusters are assigned to a different cluster. This case
does not affect the EMM, since the states are attached to the microclusters
and thus will move with them to the new cluster.

However, if splitting cluster $s_i$ into two new clusters $s_n$ and $s_m$
is necessary, we create two states with equal
outgoing transition probabilities and a fraction of the incoming
transition probabilities by:
\begin{align*}
c_{n.}& = n_n(c_{i.}/n_i)\\
c_{.n}& = n_n(c_{.i}/n_i)\\
c_{m.}& = n_m(c_{i.}/n_i)\\
c_{.m}& = n_m(c_{.i}/n_i) 
\end{align*}

These operations cover all cases typically needed to incorporate EMM into 
a clustering algorithm. Next we introduce a simple data stream clustering 
algorithm implemented in \pkg{rEMM}.

\section{Data stream clustering}

Here we use for clustering a simple density-based
clustering algorithm described in the following section.

To represent micro clusters, we use the following 
information:

\begin{itemize}
\item center (centroid/pseudo medoid)
\item counts
\item initial counts
\item data structure with weight edges to other clusters. Weights are counts.
(threshold)
\end{itemize}

We do not use sum\_x and sum\_x2 like BIRCH since we use arbitrary proximity
measures and they only help for Euclidean. For Euclidean centers are still
additive (weighted).

Use centroids for Euclidean and pseudo medoids (first point) 
for other proximity measures 
(finding canonical centroids is expensive since there
might not exist a closed form solution and
all data points might be needed~\citep{stream_clust:Leisch:2006}.)

One of the clusters is the current cluster.

build creates micro clusters

\begin{enumerate}
\item Computed dissimilarities between the new data point and the
centers.
\item find the closest cluster with a dissimilarity smaller than the threshold
\item if cluster exists then assign the new point to the cluster 
\item otherwise we create a new cluster for the point.
\item update the transition count from the current cluster to the new 
cluster.
\end{enumerate}

Reclustering to get final clustering. We can use
an arbitrary algorithm (hierarchical clustering, $k$-means, $k$-medoids, etc.)
\begin{itemize}
\item keep micro clusters but
    assign to a EMM state several micro clusters 
    (clusters of arbitrary shape, see CURE). 
\item merge micro clusters into larger clusters (spherical clusters) and 
    use them.
\end{itemize}

\marginpar{Discuss related methods (MM, HMM, AMM, ...)}

\section{Implementation details}

Class EMM, package graph and complexity.

Clustering, visualization, aging.

\section{Examples}

\subsection{Basic usage}

First, we load the package and a simple data set called {\em EMMTraffic,} which
comes with the package and was used in~\cite{emm:Dunham:2004} to illustrate
EMMs. Each observation in this hypothetical data set is a
vector of seven values obtained from sensors located at specific points on
roads. Each sensor collects a count of the number of vehicles which have
crossed this sensor in the preceding time interval.

<<>>=
library("rEMM")
data(EMMTraffic)
EMMTraffic
@

We use \func{EMM} to create a new EMM object and then build a model
using the EMMTraffic data set.
<<>>=
emm <- EMM(measure="eJaccard", threshold=0.2)
emm <- build(emm, EMMTraffic)
@

The resulting EMM has the following number of states:
<<>>=
size(emm)
@

The number of observations represented by each state can be accessed
via \func{state\_counts}. Note that the counts are only incremented when an 
outgoing transition is created. Therefore state~7 still has a count of 0.
<<>>=
state_counts(emm)
@

The state centers can be inspected using \func{state\_centers}. 
<<>>=
state_centers(emm)
@

\pkg{rEMM} has several visualization options for EMM models. For example,
as a graph.

<<Traffic_graph, fig=TRUE, include=FALSE>>=
plot(emm, method="graph")
@


The resulting graph is presented in Fig.~\ref{fig:Traffic_graph}. In this 
representation the vertices size and the arrow width code for the number of 
observations represented by each state and the transition probabilities.
However, the position of the vertices is chosen to optimize the
layout of the graph. 


\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Traffic_graph}
\caption{An EMM representing the EMMTraffic data set represented as a graph.} 
\label{fig:Traffic_graph}
\end{figure}


The transition probabilities of the EMM model can be calculated using 
\func{transition\_matrix}.
<<>>=
transition_matrix(emm)
@

Alternatively we can get also counts or log odds instead of probabilities.
<<>>=
transition_matrix(emm, type="counts")
#transition_matrix(emm, type="log_odds")
@

%Log odds are calculated as $ln(a/(1/n))$ where $a$ is the probability of
%the transition and $n$ is the number of states in the EMM.
%$1/n$ is the probability of a transition under the null model
%which assumes that the transition probability from each state
%to each other state (including staying in the same state) is the same, i.e.,
%the null model has a transition matrix with all entries equal to $1/n$.
Individual transition probabilities can be obtained 
more efficiently via \func{transition}.

<<>>=
transition(emm, "1", "2", type="probability")
@


Using the EMM model, we can predict a future state given a current state. 
From the transition matrix $\mathbf{A}$ we calculate a $\mathbf{A}^n$
which is contains the probability of moving from one state to another
state in $n$ time steps. For
example, from the state we can predict the state in $n=2$ steps. We can also
get the probability distribution of over all states (we predicted the state with
the highest probability).
<<>>=
predict_state(emm, n=2, current="2")
predict_state(emm, n=2, current="2", probabilities=TRUE)
@

State~4 was predicted since it has the highest probability. If several states
have the same probability one state is randomly chosen.

\subsection{Manipulating EMMs}
The states of EMMs can be manipulated by removing states or transitions and 
by merging states.
Fig.~\ref{fig:Traffic_man}(a) shows 
again the EMM for the EMMTraffic data set. We can remove a state
with \func{remove\_states}. For example, we remove state~3 and
display the resulting EMM in Fig~\ref{fig:Traffic_man}(b).

<<Traffic_r3, fig=TRUE, include=FALSE>>=
emm_r3 <- remove_states(emm, "3")
plot(emm_r3, "graph")
@

Removing transitions is done with \func{remove\_transitions}.
In the following example we remove the transition from state~5 to state~2.
The resulting graph is shown in Fig~\ref{fig:Traffic_man}(c).

<<Traffic_rt52, fig=TRUE, include=FALSE>>=
emm_rt52 <- remove_transitions(emm, "5", "2")
plot(emm_rt52, "graph")
@

States can be merged using \func{merge\_states}. Here we merge 
states~2 and 5 into a combined state. 
The combined state automatically gets the name of the first state in the 
merge vector. The resulting EMM is shown in Fig~\ref{fig:Traffic_man}(c).
Note that a transition from the combined state (2) is created which 
represents the transition from state~5 to state~2 in the original EMM. 


<<Traffic_m25, fig=TRUE, include=FALSE>>=
emm_m25 <- merge_states(emm, c("2","5"))
plot(emm_m25, "graph")
@

\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_r3}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_rt52}
\\(c)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_m25}
\\(d)
\end{minipage}
\caption{Graph representation for an EMM for the EMMTraffic data set.
(a) shows the original EMM, in (b) state~3 was removed, in 
(c) the transition from state~5 to state~2 was removed, and in
(d) states~2 and 5 are merged.}
\label{fig:Traffic_man}
\end{figure}


\subsection{Using a learning rate and pruning}

EMMs can adapt to changes in data over time. This is achieved by reducing
the weight of old observations in the data stream over time. We use 
a learning rate $\lambda$ to specify the weight over time. The weight
for data that is $t$ timesteps in the past is

\begin{equation}
w_t = 2^{-\lambda t}.
\end{equation}

The learning rate is specified when the EMM is created. Every insertion of
a new observation means a new timestep. So the latest added observation
has weight $1$, the observation before $2^{-\lambda}$, etc. Since we do
not store observations but only states and transitions, all the weighing is
incorporated into the counts for states and transitions.

Here we learn an EMM for the EMMTraffic data with a rather high learning
rate of $\lambda=1$ this means that the observations are weighted 
$1, \frac{1}{2}, \frac{1}{4},\dots$.

<<Traffic_l, fig=TRUE, include=FALSE>>=
emm_l <- EMM(measure="eJaccard", threshold=0.2, lambda = 1)
emm_l <- build(emm_l, EMMTraffic)
plot(emm_l, "graph")
@

The resulting graph is shown in Fig.~\ref{fig:Traffic_learning}(b) 
(for comparison Fig.~\ref{fig:Traffic_learning}(a) contains again the original
EMM). 

Over time states in an EMM can become obsolete and no new observations
are assigned to them. Similarly transitions might become obsolete over time.
To simplify the model and improve efficiency, such obsolete states and
transactions can be pruned. For the example here, we prune all states 
and transitions which have a weighted count of less than $0.1$ and
show the resulting model in Fig.~\ref{fig:Traffic_learning}(c).

<<Traffic_lp, fig=TRUE, include=FALSE>>=
emm_lp <- prune(emm_l, count_threshold=0.1)
plot(emm_lp, "graph")
@

With \func{prune} is possible to only prune states or transitions.
A more sophisticated learning scheme is possible by explicitly
calling the function
\func{age} instead of setting $\lambda$ when the EMM is created.
That way the user can specify when a timestep occurs, for example, after 
every fifth observation or following real time instead of the stream
of observations.


\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_l}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.7\linewidth]{rEMM-Traffic_lp}
\\(c)
\end{minipage}
\caption{Graph representation for an EMM for the EMMTraffic data set.
(a) shows the original EMM. (b) shows an EMM with a learning rate of
$\lambda=1$. 
(c) EMM with learning rate after pruning with a count threshold of $0.1$.}
\label{fig:Traffic_learning}
\end{figure}

\subsection{Visualization options}

We use a simulated data set called {\em EMMsim} which is included in \pkg{rEMM}.
The data contains four clusters in $\mathbb{R}^2$. Each cluster is
represented by a bivariate normally distributed random variable $X_i
\sim N_2(\mu, \Sigma)$. $\mu$ are the coordinates of the mean of the
distribution and $\Sigma$ is the covariance matrix.
The clusters are well separated.


The temporal structure of the data is modeled by the fixed sequence 
$<1,2,1,3,4>$
through the four clusters
which is 
repeated 40 times (200 data points) for the training data set 
and 5 times (25 data points) for the 
test data.

<<>>=
data("EMMsim")
@

Since the data set is in $2$-dimensional space, 
we can directly visualize the data set as a scatter plot 
(see Fig.~\ref{fig:sim_data}). We also add
the test sequence. To show the temporal structure, the points in the
test data are numbered and lines
connecting the point in sequential order.

<<sim_data, fig=TRUE, include=FALSE>>=
plot(EMMsim_train, col="gray", pch=EMMsim_sequence_train)
lines(EMMsim_test, col ="gray")
points(EMMsim_test, col="red", pch=5)
text(EMMsim_test, labels=1:nrow(EMMsim_test), pos=3)
@

\begin{figure}
\centering
%\begin{minipage}[b]{.49\linewidth}
%\centering
\includegraphics[width=.5\linewidth]{rEMM-sim_data}
%\end{minipage}
\caption{Simulated data with four clusters. The test data is plotted in red
and the temporal structure is depicted by lines between the data points.}%
\label{fig:sim_data}
\end{figure}

<<>>=
emm <- EMM(measure="euclidean", threshold=0.1)
emm <- build(emm, EMMsim_train)
@

We can visualize the resulting EMM as a graph.
<<sim_graph, fig=TRUE, include=FALSE>>=
plot(emm, method="graph")
@


The position of the vertices of the graph is solely chosen to optimize the
layout (see Fig.~\ref{fig:sim_graph}(a)). 
However, the position of the vertices can
be used to represent the dissimilarity between the centers of the states they
represent.

<<sim_MDS, fig=TRUE, include=FALSE>>=
plot(emm)
@

This results in the visualization in Fig.~\ref{fig:sim_graph}(b) which shows
the same EMM graph but the position of the vertices was determined in a way to
preserve the dissimilarity information between the centers of the states they
represent as much as possible.  For data with higher dimensionality, a
$2$-dimensional layout is computed using multidimensional scaling (MDS).

We can also project the points in the data set 
into $2$-dimensional space and then add the centers of the states 
(see Fig.~\ref{fig:sim_graph}(c)).

<<sim_MDS2, fig=TRUE, include=FALSE>>=
plot(emm, data=EMMsim_train)
@


\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_graph}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_MDS}
\\(b)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_MDS2}
\\(c)
\end{minipage}
\caption{Representation of the EMM for the simulated data. 
(a) As a simple graph. 
(b) A graph using vertex placement to represent dissimilarities.
(c) Projection of state centers onto the simulated data.
}%
\label{fig:sim_graph}
\end{figure}

The simple graph representation in Fig.~\ref{fig:sim_graph}(a) shows a rather
complicated graph for the EMM. However, 
Fig.~\ref{fig:sim_graph}(b) with the vertices positioned
to represent dissimilarities between state centers shows more structure.
The states clearly fall into four groups.
The projection of the state centers onto the data set in
Fig.~\ref{fig:sim_graph}(c) shows that the four groups represent the four
clusters in the data where the larger clusters contain more states.

\subsection{Scoring new sequences}

A scores of how likely it is that a sequence was generated by
a given EMM model can be calculated by the normalized product or the sum of the
probabilities on the path along the new sequence. For this example, we
calculate how well the test data fits the EMM.
<<>>=
score(emm, EMMsim_test, method="prod")
score(emm, EMMsim_test, method="sum")
@

Even though the test data is generated using exactly the same model as 
the training data, 
the normalized product produces a score of 0 and the normalized sum is
also very low. To analyze the problem we can look at the transition
table for the test sequence.

<<>>=
score(emm, EMMsim_test, transition_table=TRUE)
@

The low score is caused by missing transitions in the matching 
sequence of states. These missing transitions are the result of the 
fragmentation of the real clusters into many micro-clusters
(see Figs.~\ref{fig:sim_graph}(b) and (c)). Suppose 
we have two clusters called cluster A and cluster B and after an 
observation in cluster A always an observation in cluster B follows. If
now cluster A and cluster B are represented by many micro clusters each,
the chances are high that we find a pair of micro clusters (one in A and
one in B) for which we did not see a transition.
\marginpar{start with count 1 or prior prob distr.}

%\subsection{Finding the optimal threshold}
%
%We can create multiple EMMs using different thresholds and calculate the score
%on the test data for each model. The optimal threshold is then the threshold
%which produces the highest score on the test data.
%
%We use again the simulated data from above for the following example.
%First we create EMMs for all thresholds 
%from $0.2$ to $0.7$ in $0.05$ increments using the training data.
%
%<<>>=
%## find best predicting model (threshold)
%sq <- seq(0.2,0.7, by=0.05)
%emmt <- lapply(sq, FUN=function(t) {
%        emm <- EMM(measure="euclidean", threshold=t)
%        build(emm, EMMsim_train)
%    })
%names(emmt) <- sq
%@
%
%The number of states of the models is decreasing with an increasing 
%dissimilarity threshold.
%<<>>=
%sapply(emmt, size)
%@
%
%Next we score the test data against all models.
%<<>>=
%st <- sapply(emmt, score, EMMsim_test)
%st
%@
%
%The best performing model (with the smallest threshold) 
%has a score of \Sexpr{round(max(st),3)}
%and uses a threshold of \Sexpr{names(st[which.max(st)])}.
%This model is depicted in Fig.~\ref{fig:sim_optt}.
%It perfectly finds the original structure with 4 clusters. 
%
%<<sim_optt_graph, fig=TRUE, include=FALSE, echo=FALSE>>=
%plot(emmt[[which.max(st)]], method="graph")
%@
%<<sim_optt_MDS, fig=TRUE, include=FALSE, echo = FALSE>>=
%plot(emmt[[which.max(st)]], data=EMMsim_train)
%@
%
%\begin{figure}[tbp]
%\begin{minipage}[b]{.49\linewidth}
%\centering
%\includegraphics[width=.9\linewidth]{rEMM-sim_optt_graph}\\
%(a)
%\end{minipage}
%\begin{minipage}[b]{.49\linewidth}
%\centering
%\includegraphics[width=\linewidth]{rEMM-sim_optt_MDS}\\
%(b)
%\end{minipage}
%\caption{Best performing EMM with a threshold of 
%\Sexpr{names(st[which.max(st)])}}
%\label{fig:sim_optt}
%\end{figure}
%

\subsection{Reclustering states}

For this example, we learn an EMM with a small threshold, and then 
recluster the states of the EMM to find a final clustering.
We use hierarchical clustering with average linkage between state centers.
Then  states in the same cluster are merged to create the clustered
EMM. For the clustering we have to specify
$k$, the number of clusters. To find the optimal number of clusters,
we  create clustered EMMs for 
different values of $k$
and then score
the resulting models using the test data.

<<>>=
## use hierarchical clustering to find best model
emm <- EMM(measure="euclidean", threshold=0.1)
emm <- build(emm, EMMsim_train)
@


We use \func{recluster\_hclust} to create a list of clustered 
EMMs for $k=2,3,\dots,10$ (hierarchical clustering with average link). 
The attribute \code{cluster\_info} contains
information about the clustering including the dendrogram
which is shown in Fig~\ref{fig:sim_hc}.

<<sim_hc, fig=TRUE, include=FALSE>>=
## find best predicting model (clustering)
sq <- 2:10
emmc <- recluster_hclust(emm, k=sq, method ="average") 
plot(attr(emmc, "cluster_info")$dendrogram)
@

\begin{figure}[tbp]
\centering
\includegraphics[width=.5\linewidth]{rEMM-sim_hc}
\caption{Dendrogram for clustering state centers of the EMM build from 
simulated data.}
\label{fig:sim_hc}
\end{figure}

<<>>=
sc <- sapply(emmc, score, EMMsim_test)
names(sc) <- sq
sc
@

The best performing model has a score of \Sexpr{round(max(sc),3)}
and a $k$ of \Sexpr{names(sc[which.max(sc)])}.
This model is depicted in Fig.~\ref{fig:sim_optc}.

<<sim_optc_graph, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmc[[which.max(sc)]], method="graph")
@
<<sim_optc_MDS, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(emmc[[which.max(sc)]], data=EMMsim_train)
@

\begin{figure}[tbp]
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=.9\linewidth]{rEMM-sim_optc_graph}\\
(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-sim_optc_MDS}\\
(b)
\end{minipage}
\caption{Best performing EMM with a $k$ of 
\Sexpr{names(sc[which.max(sc)])}.}
\label{fig:sim_optc}
\end{figure}

Clustering also finds the original structure with 4 clusters
(see Fig.~\ref{fig:sim_optc}) with all points assigned to the correct
state. 

<<>>=
emmc[[which.max(sc)]]$var_thresholds
@

Compared with the representation in Fig.~\ref{fig:sim_optc}(b) we can see that
states representing more dispersed clusters of points (e.g. state~1) were
assigned larger thresholds than more compact clusters (e.g. state~5).


%Use the Boost Graph Library to check if the found optimal EMMs
%have the same structure
%<<>>=
%library("RBGL")
%isomorphism(emmt[[which.max(st)]]$mm,emmc[[which.max(sc)]]$mm)
%@


\section{Applications}

\subsection{Analyzing river flow data}
The \pkg{rEMM} package also contains a data set called {\em Derwent} which was
originally used by~\cite{emm:Dunham:2004}.  It contains river flow readings
(measured in $m^3$ per second) from six catchments of in the river Derwent and
two of its main tributaries in the northern England.  The data was collected
daily for roughly 5 years from November 1, 1971 to January 31, 1977.  The
catchments are Long Bridge, Matlock Bath, Chat Sworth, What Stand Well, Ashford
(river Wye) and Wind Field Park (river Amber).

The data set is interesting since it contains annual changes of
river levels and also some special flooding events.

<<>>=
data(Derwent)
summary(Derwent)
@

From the summary we see that the average flows vary for the catchments
significantly (from 0.143 to 14.238).  The influence of differences in averages
flows can be removed by scaling the data before building the EMM.  Form the
summary we also see that for the Ashford and Wind Filed Park catchments a
significant amount of observations is not available.  EMM deals with these
missing values by using only the non-missing dimensions of the observations for
the dissimilarity calculations. 

<<Derwent1, fig=TRUE, include=FALSE>>=
plot(Derwent[,1], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[1])
@
\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Derwent1}
\caption{Gauged flows (in $m^3/s$) of the river Derwent at the 
Long Bridge catchment.} 
\label{fig:Derwent1}
\end{figure}

In Fig.~\ref{fig:Derwent1} we can see the annual flow structure for the Long
Bridge catchment with high flows in September to March and lower flows in the
summer months. The first year seams to have more variability in the summer
months and the second year has an unusual event (around the index of 600 in
Fig.~\ref{fig:Derwent1}) with a flow above $100 m^3/s$ which can be classified
flooding.

We build an EMM from the (centered and) scaled river data using Euclidean
distance between the vectors containing the flows from the six catchments and
experimentally found a distance threshold of 3 (just above the
3rd quartile of the distance distribution between all scaled observations) 
to give useful results.

<<Derwent_state_counts, fig=TRUE, include=FALSE, width=10>>=
Derwent_scaled <- scale(Derwent)
emm <- EMM(measure="euclidean", threshold=3)
emm <- build(emm, Derwent_scaled)
#state_counts(emm)
#state_centers(emm)
plot(emm, "state_counts", log="y")
@
\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{rEMM-Derwent_state_counts}
\caption{Distribution of state counts of the EMM for the Derwent data.} 
\label{fig:Derwent_state_counts}
\end{figure}

The resulting EMM has \Sexpr{size(emm)} states. In
Fig.~\ref{fig:Derwent_state_counts} shows that the counts for the states have a
very skewed distribution with states~1 and 2 representing most observations. 

<<Derwent_EMM1, fig=TRUE, include=FALSE>>=
plot(emm)
@
\begin{figure}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-Derwent_EMM1}
\\(a)
\end{minipage}
\begin{minipage}[b]{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{rEMM-Derwent_EMM2}
\\(b)
\end{minipage}
\caption{State centers of the EMM for the Derwent data set projected
on $2$-dimensional space.
(a) shows the full EMM and (b) shows a reduced EMM (only the most 
frequently used states)}
\label{fig:Derwent_EMM}
\end{figure}

The projection of the state centers into $2$-dimensional space in
Fig.~\ref{fig:Derwent_EMM}(a) reveals that all states except state~11 and 12
are placed close together and are highly connected. Other states (9, 10, 17 and
19) also seem somewhat different from the majority of states.

Next we look at frequent states. We define rare states here as all 
states that represent less than 0.5\% of the observations. 
On average this translates into less than two daily observation per year.
We calculate a count threshold and then plot the EMM with only the
states which are frequent. For the plot we remove the transitions
from each state to itself to see the most important outgoing transitions
more clearly.

<<Derwent_EMM2, fig=TRUE, include=FALSE>>=
rare_threshold <- sum(state_counts(emm))*0.005
rare_threshold
#plot(prune(emm, rare_threshold, transitions=FALSE))
plot(remove_selftransitions(prune(emm, rare_threshold, transitions=FALSE)))
@

The reduced model depicted in Fig.~\ref{fig:Derwent_EMM}(b) shows that 5 states
represent approximately 99.5\% of the river's behavior.  States~1 and 2 are the
most frequently used states and have a wide arrow (representing transition
probabilities) going both directions between them.
%<<>>=
%rare <- names(which(state_counts(emm)<rare_threshold))
%rare
%@
To analyze the meaning of the two outlier states (11 and 12) 
identified in Fig.~\ref{fig:Derwent_EMM}(a) above, we plot the flows at a 
catchment and mark the observations for these states. 

<<Derwent2, fig=TRUE, include=FALSE>>=
catchment <- 1 
plot(Derwent[,catchment], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[catchment])
state_sequence <- find_states(emm, Derwent_scaled)

mark_states <- function(states, state_sequence, ys, col=0, label=NULL, ...) {
    x <- which(state_sequence %in% states)
    points(x, ys[x], col=col, ...)
    if(!is.null(label)) text(x, ys[x], label, pos=4, col=col)
}

mark_states("11", state_sequence, Derwent[,catchment], col="blue", label="11")
mark_states("12", state_sequence, Derwent[,catchment], col="red", label="12")
#mark_states("9", state_sequence, Derwent[,catchment], col="green", label="9")
#mark_states("3", state_sequence, Derwent[,catchment], col="blue")
@
\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Derwent2}
\caption{Gauged flows (in $m^3/s$) of the river Derwent at the 
Long Bridge catchment.} 
\label{fig:Derwent2}
\end{figure}

In Fig.~\ref{fig:Derwent2} we see that state~12 has a river flow in excess of
$100 m^3/s$ which only happened once in the observation period. The state~11
seems to be a regular observation with medium flow around $20 m^3/s$ and it
needs more analysis to find out why this state is also an outlier directly
leading to state~12.

<<Derwent3, fig=TRUE, include=FALSE>>=
catchment <- 6 
plot(Derwent[,catchment], type="l", ylab="Gauged flows", 
main=colnames(Derwent)[catchment])

mark_states("11", state_sequence, Derwent[,catchment], col="blue", label="11")
mark_states("12", state_sequence, Derwent[,catchment], col="red", label="12")
#mark_states("9", state_sequence, Derwent[,catchment], col="green", label="9")
#mark_states("3", state_sequence, Derwent[,catchment], col="blue")
@
\begin{figure}
\centering
\includegraphics[width=.5\linewidth]{rEMM-Derwent3}
\caption{Gauged flows (in $m^3/s$) of the river Amber at the 
Wind Field Park catchment.} 
\label{fig:Derwent3}
\end{figure}

The catchment at Wind Field Park is at the Amber river which is a tributary of
the Derwent and we see in Fig.~\ref{fig:Derwent3} that the day before the flood
occurs, the flow shoots up to $4 m^3/s$ which is caught by state~11 and gives a
clear sign that a flood is imminent the next day.
 
%Next we look at the probability of going from state~1 (normal condition)
%to state~12 (flooding) in $n=1,2,\dots,10$ days.
%
%<<>>=
%n <- 1:10
%probs <- sapply(n, FUN = function(n) predict_state(emm, n=n, 
%current="1", probabilities=TRUE))[12,]
%names(probs) <- n
%probs
%@
%
%From the probabilities we see that we cannot go directly from 1 to 12.
%Only after two steps we can get to state~12 and the probability is then
%between 0.0005 and 0.0007.
%
%<<fig=TRUE>>=
%hc <- cluster_states(emm)
%plot(hc)
%@
%<<fig=TRUE>>=
% emm20 <- merge_states(emm, cutree(hc, h=20), clustering=TRUE)
% state_centers(emm20)
% plot(remove_selftransitions(emm20))
%@
%<<fig=TRUE>>=
%plot(remove_selftransitions(emm20), "graph")
%@

\subsection{Genetic sequence analysis}
The \pkg{rEMM} package contains data for 16S ribosomal RNA (rRNA)
sequences for the two phylogenetic classes, Alphaproteobacteria and Mollicutes. 
16S rRNA is a component of the ribosomal subunit 30S and is regularly 
used for phylogenetic studies~\citep[e.g., see][]{rna:Wang:2007}. 
Typically alignment heuristics like BLAST~\citep{rna:Altschul:1990}
or a Hidden Markov Model (HMM)~\citep[e.g.,][]{rna:Hughey:1996} are used
for evaluating the similarity between two or more sequences. However,
these procedures are computationally very expensive. 

An alternative approach is to describe the structure in terms of the occurrence
frequency of so called $n$-words, subsequences of length $n$. Counting the
occurrences of the $4^n$ (there are four bases) $n-words$ is straight forward
and computing similarities between the two frequency profiles if very
efficient. Because no alignment is computed, such methods are called
alignment-free~\citep{rna:Vinga:2003}. 
%A direct application of this
%approach is cd-hit, a method that finds identical subsequences by
%first evaluating the number of matching $n$-word frequencies and only 
%if the 

Here, the sequence data for 30
16S sequences of the phylogenetic class Mollicutes
was preprocessed by cutting the sequences into windows 
of length 100 nucleotides (bases) without overlap and then for each window
the occurrence of triplets of nucleotides was counted resulting in $4^3=64$
counts per window. Each window will be used as an observation
to build the EMM.

\cite{rna:Vinga:2003} review dissimilarity measures used for alignment-free
methods. They most commonly used measures are Euclidean distance, $d^2$ distance
(a weighted Euclidean distance), Mahalanobis distance, Kullback-Leibler
discrepancy (KLD). Since \cite{rna:Wu:2001} find in their experiments that
KLD provides good results while it still can
be computed as fast as Euclidean distance, it is also used here.
Since KLD becomes $-\infty$ for counts of zero, we
add one to all counts which conceptually means that we start building 
the EMM with a prior that all triplets have the equal occurrence 
probability~\cite[see][]{rna:Wu:2001}.
<<>>=
data("16S")

emm <- EMM("Kullback", threshold=0.1)
emm <- build(emm, Mollicutes16S+1)
@

<<Mollicutes_graph, fig=TRUE, include=FALSE>>=
plot(emm, "graph", parameter=list(transition_probabilities=FALSE))
## start state for sequences have an initial state probability >0
it <- initial_transition(emm)
it[it>0]
@

\begin{figure}
\centering
\includegraphics[width=\linewidth]{rEMM-Mollicutes_graph}
\caption{An EMM representing 16S sequences from the class Mollicutes 
represented as a graph.} 
\label{fig:Mollicutes_graph}
\end{figure}

The graph representation of the EMM is shown in 
Fig.~\ref{fig:Mollicutes_graph}
Note that each state in the EMM corresponds to one or more windows 
of the rRNA sequence (the size of the state indicates the number 
of windows). The initial transition probabilities show that all sequences start with the first window in state~1
Several interesting observations can be made from this 
representation.
\begin{itemize}
\item There exists a path through the graph using only the largest states
    which represents the most common sequence of windows.
\item There are several places the EMM where all sequences converge (e.g., 14)
\item There are places where many possible parallel paths exist 
    (e.g., 32, 35, 7, 19, 27,45)
\item The windows composition changes over the sequences since there are
    only a few edges going back (e.g., from 24 to 2) or skipping states 
    on the way down (e.g., from 2 to 11).
\end{itemize}

In general it is interesting that the graph has not more loops since 
\cite{rna:Deschavanne:1999} found in their study using Chaos Game 
Representation that the variability along genomes and among genomes is low.
However, they looked at longer sequences and we look here at the micro 
structure of a very short sequence.
These observations merit closer analysis by biologists.

\section{Conclusion}

To come

\bibliographystyle{abbrvnat}
\bibliography{rEMM,rna_sequences,stream_clust}
\end{document}
